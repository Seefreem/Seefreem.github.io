# 欢迎访问邓世玲的主页 (Welcome to Shiling's Pages)

## Summary of my GitHub
### [university_of_copenhagen](https://github.com/Seefreem/university_of_copenhagen)
This Repo has assignments I have had at the University of Copenhagen.  

### [ITU_ANLPDL_Final](https://github.com/Seefreem/ITU_ANLPDL_Final)
Project: Uncertainty Prediction: An Exploratory Experiment on Representation Engineering Approach  
This project is a mini research project of the course Advanced Natural Language Processing and Deep Learning at the IT University of Copenhagen.   
This study explores a representation-distribution-based approach using Gaussian Mixture Models (GMMs) to estimate confidence in LLM responses by modeling last-token representation distributions to distinguish between known and unknown answers. Experiments on CoQA and TriviaQA show that GMMs perform well on factual tasks but struggle with language comprehension.  

### [KU_ATNLP_Individual_Assignment](https://github.com/Seefreem/KU_ATNLP_Individual_Assignment)
Project: Using BERT for Text Generation and Assessing Its Generalization Ability   
This project is the final exam assignment of the course Advanced Topics in Natural Language Processing at the University of Copenhagen.   
Studied the text generation and compositional abilities of BERT. Developed Generative-BERT (GBERT), a novel approach leveraging masked language modeling to generate full sentences. Experiments on the SCANdataset reveal GBERT's near-perfect accuracy on the training distribution but limited extrapolation ability.

### [meme_text_retrieval_p1](https://github.com/Seefreem/meme_text_retrieval_p1)
This repo contains the source code for my project outside course scope at Pioneer Centre for Artificial Intelligence, Denmark.  
Project title: Meme-text retrieval: a new dataset and a cross-model embedder  
Main supervisor: Serge Belongie  
Co-supervisor: Peter Ebert Christensen  

### [meme_text_retrieval](https://github.com/Seefreem/meme_text_retrieval)
This project is the final exam assignment of the course HIOK0003FU Computational Cognitive Science 2 at the University of Copenhagen.  
We have explored the effectiveness of cross-modal embedding models, specifically focusing on their application in meme-text retrieval scenarios.  
Group members: Shiling Deng, Qing Li and Yuwei Shen

### [Multi-author-style-change](https://github.com/avialofmeth/Multi-Author-Style-Change)
This project is the final exam assignment of HIOK0002FU Language Processing 2 at the University of Copenhagen.
In this repo, we employed three approaches: Prompt engineering, BERT and TF-IDF (Baseline).  
Group members: Yuwei Shen, Qing Li and Shiling Deng  

### [pre-training LLMs](https://github.com/Seefreem/llm_pre_training_acceleration)
In this repository, I explored teacher-student pretraining (knowledge distillation). I utilized a bi-gram model as the teacher and a smaller LLaMA-2 model as the student. By gradually adjusting the influence of the teacher model during training, the student model showed a slight improvement in performance on the validation dataset.  

### [LLM](https://github.com/Seefreem/LLM)
This repo is about Large Language Models. It holds some of my notes and experiments.  

### [KnowledgeBase](https://github.com/Seefreem/KnowledgeBase)
This repo serves as a handbook.   

